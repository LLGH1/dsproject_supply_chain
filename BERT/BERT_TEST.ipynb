{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/LLGH1/TEST/blob/main/BERT_TEST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6z8lJGXMg-kT"
   },
   "source": [
    "# init google drive and git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sGpQvuOHbbS_",
    "outputId": "b3bba03a-1fae-4763-c1b3-0bd30256c542"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch main\r\n",
      "Your branch is up to date with 'origin/main'.\r\n",
      "\r\n",
      "Changes not staged for commit:\r\n",
      "  (use \"git add <file>...\" to update what will be committed)\r\n",
      "  (use \"git checkout -- <file>...\" to discard changes in working directory)\r\n",
      "\r\n",
      "\t\u001b[31mmodified:   BERT_TEST.ipynb\u001b[m\r\n",
      "\r\n",
      "Untracked files:\r\n",
      "  (use \"git add <file>...\" to include in what will be committed)\r\n",
      "\r\n",
      "\t\u001b[31m.ipynb_checkpoints/\u001b[m\r\n",
      "\r\n",
      "no changes added to commit (use \"git add\" and/or \"git commit -a\")\r\n"
     ]
    }
   ],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P4PGotwUhTx9"
   },
   "source": [
    "# BERT-TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s8eBNesrZkQU"
   },
   "source": [
    "[link text](https://)Reference: \n",
    "BERT Fine-Tuning Sentence Classification.ipynb > https://colab.research.google.com/drive/1ywsvwO6thOVOrfagjjfuxEf6xVRxbUNO#scrollTo=68xreA9JAmG5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8qhOCUfzPkCA",
    "outputId": "1f02af2d-63c9-4de4-8d82-815ca7dfb376"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-pretrained-bert\n",
      "  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n",
      "     -------------------------------------- 123.8/123.8 kB 7.1 MB/s eta 0:00:00\n",
      "Collecting pytorch-nlp\n",
      "  Downloading pytorch_nlp-0.5.0-py3-none-any.whl (90 kB)\n",
      "     ---------------------------------------- 90.1/90.1 kB ? eta 0:00:00\n",
      "Requirement already satisfied: numpy in c:\\users\\eqdiyci\\onedrive - allianz\\trainings\\datascience_academy\\project\\ds_project\\lib\\site-packages (from pytorch-pretrained-bert) (1.24.2)\n",
      "Collecting torch>=0.4.1\n",
      "  Downloading torch-2.0.0-cp310-cp310-win_amd64.whl (172.3 MB)\n",
      "     -------------------------------------- 172.3/172.3 MB 7.0 MB/s eta 0:00:00\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.26.107-py3-none-any.whl (135 kB)\n",
      "     ---------------------------------------- 135.6/135.6 kB ? eta 0:00:00\n",
      "Requirement already satisfied: tqdm in c:\\users\\eqdiyci\\onedrive - allianz\\trainings\\datascience_academy\\project\\ds_project\\lib\\site-packages (from pytorch-pretrained-bert) (4.65.0)\n",
      "Requirement already satisfied: requests in c:\\users\\eqdiyci\\onedrive - allianz\\trainings\\datascience_academy\\project\\ds_project\\lib\\site-packages (from pytorch-pretrained-bert) (2.28.2)\n",
      "Requirement already satisfied: regex in c:\\users\\eqdiyci\\onedrive - allianz\\trainings\\datascience_academy\\project\\ds_project\\lib\\site-packages (from pytorch-pretrained-bert) (2023.3.23)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\eqdiyci\\onedrive - allianz\\trainings\\datascience_academy\\project\\ds_project\\lib\\site-packages (from torch>=0.4.1->pytorch-pretrained-bert) (4.5.0)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.10.7-py3-none-any.whl (10 kB)\n",
      "Collecting sympy\n",
      "  Downloading sympy-1.11.1-py3-none-any.whl (6.5 MB)\n",
      "     ---------------------------------------- 6.5/6.5 MB 37.6 MB/s eta 0:00:00\n",
      "Collecting networkx\n",
      "  Downloading networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "     ---------------------------------------- 2.1/2.1 MB 43.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: jinja2 in c:\\users\\eqdiyci\\onedrive - allianz\\trainings\\datascience_academy\\project\\ds_project\\lib\\site-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.1.2)\n",
      "Collecting botocore<1.30.0,>=1.29.107\n",
      "  Downloading botocore-1.29.107-py3-none-any.whl (10.6 MB)\n",
      "     --------------------------------------- 10.6/10.6 MB 38.4 MB/s eta 0:00:00\n",
      "Collecting jmespath<2.0.0,>=0.7.1\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting s3transfer<0.7.0,>=0.6.0\n",
      "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
      "     ---------------------------------------- 79.6/79.6 kB 4.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\eqdiyci\\onedrive - allianz\\trainings\\datascience_academy\\project\\ds_project\\lib\\site-packages (from requests->pytorch-pretrained-bert) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\eqdiyci\\onedrive - allianz\\trainings\\datascience_academy\\project\\ds_project\\lib\\site-packages (from requests->pytorch-pretrained-bert) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\eqdiyci\\onedrive - allianz\\trainings\\datascience_academy\\project\\ds_project\\lib\\site-packages (from requests->pytorch-pretrained-bert) (3.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\eqdiyci\\onedrive - allianz\\trainings\\datascience_academy\\project\\ds_project\\lib\\site-packages (from requests->pytorch-pretrained-bert) (1.26.15)\n",
      "Requirement already satisfied: colorama in c:\\users\\eqdiyci\\onedrive - allianz\\trainings\\datascience_academy\\project\\ds_project\\lib\\site-packages (from tqdm->pytorch-pretrained-bert) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\eqdiyci\\onedrive - allianz\\trainings\\datascience_academy\\project\\ds_project\\lib\\site-packages (from botocore<1.30.0,>=1.29.107->boto3->pytorch-pretrained-bert) (2.8.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\eqdiyci\\onedrive - allianz\\trainings\\datascience_academy\\project\\ds_project\\lib\\site-packages (from jinja2->torch>=0.4.1->pytorch-pretrained-bert) (2.1.2)\n",
      "Collecting mpmath>=0.19\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "     ------------------------------------- 536.2/536.2 kB 35.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\eqdiyci\\onedrive - allianz\\trainings\\datascience_academy\\project\\ds_project\\lib\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.107->boto3->pytorch-pretrained-bert) (1.16.0)\n",
      "Installing collected packages: mpmath, sympy, networkx, jmespath, filelock, torch, pytorch-nlp, botocore, s3transfer, boto3, pytorch-pretrained-bert\n",
      "Successfully installed boto3-1.26.107 botocore-1.29.107 filelock-3.10.7 jmespath-1.0.1 mpmath-1.3.0 networkx-3.1 pytorch-nlp-0.5.0 pytorch-pretrained-bert-0.6.2 s3transfer-0.6.0 sympy-1.11.1 torch-2.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# !pip install pytorch-pretrained-bert pytorch-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t6v0fYeYP6Wf",
    "outputId": "ddd2cf7e-c710-4738-f0d7-e3295bc5b466"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Downloading keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
      "     ---------------------------------------- 1.7/1.7 MB 37.6 MB/s eta 0:00:00\n",
      "Installing collected packages: keras\n",
      "Successfully installed keras-2.12.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# !pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oR-GNf3sQQbI",
    "outputId": "6840735a-731b-4a0c-fe26-3dc5c770552d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras_preprocessing"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "     ---------------------------------------- 42.6/42.6 kB 2.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\eqdiyci\\onedrive - allianz\\trainings\\datascience_academy\\project\\ds_project\\lib\\site-packages (from keras_preprocessing) (1.24.2)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\eqdiyci\\onedrive - allianz\\trainings\\datascience_academy\\project\\ds_project\\lib\\site-packages (from keras_preprocessing) (1.16.0)\n",
      "Installing collected packages: keras_preprocessing\n",
      "Successfully installed keras_preprocessing-1.1.2\n"
     ]
    }
   ],
   "source": [
    "# !pip install keras_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "I5np1-bZPt4a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "# from keras.preprocessing.sequence import pad_sequences # error\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from torch import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "id": "L5G2X6_CQXuw",
    "outputId": "0d2f3c10-7fbb-44c0-9aec-9216dbba02fc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_headline</th>\n",
       "      <th>processed_reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>Quite Good</td>\n",
       "      <td>easi setup use digit input one best wireless h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>The headphones sound great; they are comfortab...</td>\n",
       "      <td>go refurbish rout worth save headphon sound gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect replacement</td>\n",
       "      <td>work great replac lost origin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Lots of action</td>\n",
       "      <td>game allow novic like world champ also let mov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Great Protection at a Great Price!</td>\n",
       "      <td>specif protector review insten brand protector...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   star_rating                                    review_headline  \\\n",
       "0            4                                         Quite Good   \n",
       "1            4  The headphones sound great; they are comfortab...   \n",
       "2            5                                Perfect replacement   \n",
       "3            4                                     Lots of action   \n",
       "4            5                 Great Protection at a Great Price!   \n",
       "\n",
       "                                   processed_reviews  \n",
       "0  easi setup use digit input one best wireless h...  \n",
       "1  go refurbish rout worth save headphon sound gr...  \n",
       "2                      work great replac lost origin  \n",
       "3  game allow novic like world champ also let mov...  \n",
       "4  specif protector review insten brand protector...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data\n",
    "df = pd.read_csv(\"data_en_sample2.csv.zip\", compression=\"zip\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NwXE6Mk1H49_",
    "outputId": "0bcdeb04-39d9-4f5b-9ed3-147e5f71c2fa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105192, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lXM0T9iDjf9Q",
    "outputId": "e11a1e9e-b267-4552-b634-0cc9695d9593"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['star_rating'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "utoEG2kdmHPT"
   },
   "outputs": [],
   "source": [
    "df['reviews'] = df['processed_reviews'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "fnLPpZIwka65"
   },
   "outputs": [],
   "source": [
    "df['reviews'] = df['reviews'].apply(lambda x: \"[CLS] \" + x[:127] + \" [SEP]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 257
    },
    "id": "o7sUfzxWml_8",
    "outputId": "baca011c-a020-4362-e9e7-4c67587ba51c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_headline</th>\n",
       "      <th>processed_reviews</th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25974</th>\n",
       "      <td>1</td>\n",
       "      <td>Didn't work for me</td>\n",
       "      <td>bought item think cant go wrong soni brand wro...</td>\n",
       "      <td>[CLS] bought item think cant go wrong soni bra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66667</th>\n",
       "      <td>4</td>\n",
       "      <td>The best of the series</td>\n",
       "      <td>edit best seri good reason felt first one hard...</td>\n",
       "      <td>[CLS] edit best seri good reason felt first on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51497</th>\n",
       "      <td>5</td>\n",
       "      <td>its' s***</td>\n",
       "      <td>zero ten star thub farm te posit aint care com...</td>\n",
       "      <td>[CLS] zero ten star thub farm te posit aint ca...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       star_rating         review_headline  \\\n",
       "25974            1      Didn't work for me   \n",
       "66667            4  The best of the series   \n",
       "51497            5               its' s***   \n",
       "\n",
       "                                       processed_reviews  \\\n",
       "25974  bought item think cant go wrong soni brand wro...   \n",
       "66667  edit best seri good reason felt first one hard...   \n",
       "51497  zero ten star thub farm te posit aint care com...   \n",
       "\n",
       "                                                 reviews  \n",
       "25974  [CLS] bought item think cant go wrong soni bra...  \n",
       "66667  [CLS] edit best seri good reason felt first on...  \n",
       "51497  [CLS] zero ten star thub farm te posit aint ca...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "PSIKaMAraYsS"
   },
   "outputs": [],
   "source": [
    "# Create reviews and rating lists\n",
    "reviews = df.reviews.values\n",
    "\n",
    "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
    "# reviews = [\"[CLS] \" + review[:511] + \" [SEP]\" for review in reviews]\n",
    "ratings = df.star_rating.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6nfgn5ut2uT8",
    "outputId": "7a09af94-9c4d-4a42-eeab-07d5bd5a79cd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['[CLS] easi setup use digit input one best wireless headset come sound make two thing wish better first microphon wireless requir cabl [SEP]',\n",
       "       '[CLS] go refurbish rout worth save headphon sound great comfort overal need readjust hour game play microphon pull nice flexibl mute  [SEP]',\n",
       "       '[CLS] work great replac lost origin [SEP]', ...,\n",
       "       '[CLS] game littl hinki playstat iii sometim work sometim work great game addict [SEP]',\n",
       "       '[CLS] bought limit edit could also play hero iii forget trick game even load save file everi time tri get second quest campaign load  [SEP]',\n",
       "       '[CLS] avoid br noth game announc box br great pc run fps br game broken save frustrat avoid [SEP]'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jfzw3BOsaHjv"
   },
   "source": [
    "## some tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "cgRwiegJbP8I"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow_text in /usr2/postdoc/fpoege/.local/lib/python3.8/site-packages (2.12.0)\n",
      "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr2/postdoc/fpoege/.local/lib/python3.8/site-packages (from tensorflow_text) (0.13.0)\n",
      "Requirement already satisfied: tensorflow<2.13,>=2.12.0 in /usr2/postdoc/fpoege/.local/lib/python3.8/site-packages (from tensorflow_text) (2.12.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr2/postdoc/fpoege/.local/lib/python3.8/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /usr2/postdoc/fpoege/.local/lib/python3.8/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (23.3.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (3.3.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr2/postdoc/fpoege/.local/lib/python3.8/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (0.32.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (1.15.0)\n",
      "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr2/postdoc/fpoege/.local/lib/python3.8/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (2.12.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr2/postdoc/fpoege/.local/lib/python3.8/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (1.53.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr2/postdoc/fpoege/.local/lib/python3.8/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (2.12.0)\n",
      "Requirement already satisfied: packaging in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (20.9)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (3.10.0.0)\n",
      "Requirement already satisfied: numpy<1.24,>=1.22 in /usr2/postdoc/fpoege/.local/lib/python3.8/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (1.23.5)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr2/postdoc/fpoege/.local/lib/python3.8/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (1.4.0)\n",
      "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr2/postdoc/fpoege/.local/lib/python3.8/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (2.12.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (3.2.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr2/postdoc/fpoege/.local/lib/python3.8/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (4.22.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr2/postdoc/fpoege/.local/lib/python3.8/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (16.0.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr2/postdoc/fpoege/.local/lib/python3.8/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (0.2.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr2/postdoc/fpoege/.local/lib/python3.8/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (0.4.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr2/postdoc/fpoege/.local/lib/python3.8/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (2.2.0)\n",
      "Requirement already satisfied: setuptools in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (56.0.0)\n",
      "Requirement already satisfied: jax>=0.3.15 in /usr2/postdoc/fpoege/.local/lib/python3.8/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (0.4.8)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text) (1.12.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow<2.13,>=2.12.0->tensorflow_text) (0.36.2)\n",
      "Requirement already satisfied: scipy>=1.7 in /usr2/postdoc/fpoege/.local/lib/python3.8/site-packages (from jax>=0.3.15->tensorflow<2.13,>=2.12.0->tensorflow_text) (1.10.1)\n",
      "Requirement already satisfied: ml-dtypes>=0.0.3 in /usr2/postdoc/fpoege/.local/lib/python3.8/site-packages (from jax>=0.3.15->tensorflow<2.13,>=2.12.0->tensorflow_text) (0.0.4)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (1.0.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr2/postdoc/fpoege/.local/lib/python3.8/site-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (0.7.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (3.3.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (2.25.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr2/postdoc/fpoege/.local/lib/python3.8/site-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (1.8.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr2/postdoc/fpoege/.local/lib/python3.8/site-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (0.4.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (1.30.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from packaging->tensorflow<2.13,>=2.12.0->tensorflow_text) (2.4.7)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr2/postdoc/fpoege/.local/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (1.3.1)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (1.26.4)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr2/postdoc/fpoege/.local/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text) (3.2.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.0.1 is available.\r\n",
      "You should consider upgrading via the '/share/pkg.7/python3/3.8.10/install/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "9MiX9e2jaF55"
   },
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "gyvX_xZNajm5"
   },
   "outputs": [],
   "source": [
    "encoder_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\"\n",
    "preprocess_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "x8zq9KzQbqzM"
   },
   "outputs": [],
   "source": [
    "bert_preprocess_model = hub.KerasLayer(preprocess_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zY0Nk_ChcELA",
    "outputId": "34ddffd3-9429-4589-d425-29642dd1dbc1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_type_ids', 'input_mask', 'input_word_ids'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_test = ['greate movie', \"awesome game\", \"it is not that good\"]\n",
    "text_preprocessed = bert_preprocess_model(text_test)\n",
    "text_preprocessed.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ugrAG0JTccrN",
    "outputId": "086e0efc-9edd-40f8-d227-e35485383dc0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 128), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "      dtype=int32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_preprocessed['input_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DY33tvlBc0Lx",
    "outputId": "a5c1a0c8-e0ff-478d-ce22-25d4ec79e626"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 128), dtype=int32, numpy=\n",
       "array([[  101,  2307,  2063,  3185,   102,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0],\n",
       "       [  101, 12476,  2208,   102,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0],\n",
       "       [  101,  2009,  2003,  2025,  2008,  2204,   102,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0]], dtype=int32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_preprocessed['input_word_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HhWajNNxaLKH"
   },
   "source": [
    "## BERT tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hr0IrmJ0div8",
    "outputId": "30480c66-22d6-491b-d862-3ad1c595817a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize the first review:\n",
      "['[CLS]', 'ea', '##si', 'setup', 'use', 'digit', 'input', 'one', 'best', 'wireless', 'heads', '##et', 'come', 'sound', 'make', 'two', 'thing', 'wish', 'better', 'first', 'micro', '##phon', 'wireless', 're', '##qui', '##r', 'cab', '##l', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# import BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in reviews]\n",
    "print (\"Tokenize the first review:\")\n",
    "print (tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "jfMhrwtXM_aI"
   },
   "outputs": [],
   "source": [
    "# tokenized_texts_truc = tolist(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:127] + ['[SEP]'], train_texts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CwzJG0-9KYx4",
    "outputId": "60cf29bf-5424-472a-9339-fc42440b4100"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105192"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_texts) # 70128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "O0hBmatsKeR9"
   },
   "outputs": [],
   "source": [
    "# for i in range(0, len(tokenized_texts)):\n",
    "#   max_seq = 0\n",
    "#   max_i = 0\n",
    "#   seq = len(tokenized_texts[i])\n",
    "#   if seq > max_seq:\n",
    "#     max_seq = seq\n",
    "#     max_i = i\n",
    "\n",
    "# print(\"max_seq: \", tokenized_texts[max_i], \"\\n\", max_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "TuF225YWdmaL"
   },
   "outputs": [],
   "source": [
    "# Set the maximum sequence length. The longest sequence in our training set is ? , but we'll leave room on the end anyway. \n",
    "MAX_LEN = 128 # max length from BERT is 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "KdJ2uAodd04R"
   },
   "outputs": [],
   "source": [
    "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "OpROtAyMd23Q"
   },
   "outputs": [],
   "source": [
    "# Pad our input tokens\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "7XYQ9Qkqd4rZ"
   },
   "outputs": [],
   "source": [
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  attention_masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "OcU4-R5Md-Id"
   },
   "outputs": [],
   "source": [
    "# Use train_test_split to split our data into train and validation sets for training\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, ratings, random_state=2018, test_size=0.2)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids, random_state=2018, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "WFrqqjc0N352"
   },
   "outputs": [],
   "source": [
    "# Convert all of our data into torch tensors, the required datatype for our model\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "eui9_Jr0OHNt"
   },
   "outputs": [],
   "source": [
    "# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
    "batch_size = 32\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
    "# with an iterator the entire dataset does not need to be loaded into memory\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IxbdcvftV2B8"
   },
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_eVPMDDdUJ0q",
    "outputId": "be66eb44-2b09-4c80-9074-7c71c4971187"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "device='cuda'\n",
    "cuda.empty_cache()\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Cbnpmt1Ogkw",
    "outputId": "82add3a7-42f6-4c1d-8112-fa34891100ee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=6) # num_labels = 5 does not work. why? \n",
    "# model.cuda() # if no cuda GPUs --> comment out\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "4GnEYaCrUGj6"
   },
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-ETd90Q0Ubya",
    "outputId": "92ddf6b8-b792-4d97-a720-5ce36e8acfe7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pytorch_pretrained_bert.optimization:t_total value of -1 results in schedule not being applied\n"
     ]
    }
   ],
   "source": [
    "# This variable contains all of the hyperparemeter information our training loop needs\n",
    "optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                     lr=2e-5,\n",
    "                     warmup=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "NJHObgsrUizx"
   },
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 557
    },
    "id": "tFQNHdVRUuUv",
    "outputId": "9ca0d244-3bff-4785-fafa-62f06566d532"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Epoch:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.9386026786647369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Epoch:  25%|██▌       | 1/4 [18:34<55:43, 1114.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6373575227963526\n",
      "Train loss: 0.841663405503157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Epoch:  50%|█████     | 2/4 [37:00<36:59, 1109.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6376298125633232\n",
      "Train loss: 0.7242307300576692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Epoch:  75%|███████▌  | 3/4 [55:25<18:27, 1107.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6301671732522796\n",
      "Train loss: 0.5940800861770662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 4/4 [1:13:50<00:00, 1107.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6139722644376899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "t = [] \n",
    "\n",
    "# Store our loss and accuracy for plotting\n",
    "train_loss_set = []\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 4\n",
    "\n",
    "# trange is a tqdm wrapper around the normal python range\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "  \n",
    "  \n",
    "  # Training\n",
    "  \n",
    "  # Set our model to training mode (as opposed to evaluation mode)\n",
    "  model.train()\n",
    "  \n",
    "  # Tracking variables\n",
    "  tr_loss = 0\n",
    "  nb_tr_examples, nb_tr_steps = 0, 0\n",
    "  \n",
    "  # Train the data for one epoch\n",
    "  for step, batch in enumerate(train_dataloader):\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    # Clear out the gradients (by default they accumulate)\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass\n",
    "    loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "    train_loss_set.append(loss.item())    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    # Update parameters and take a step using the computed gradient\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    # Update tracking variables\n",
    "    tr_loss += loss.item()\n",
    "    nb_tr_examples += b_input_ids.size(0)\n",
    "    nb_tr_steps += 1\n",
    "\n",
    "  print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "    \n",
    "    \n",
    "  # Validation\n",
    "\n",
    "  # Put model in evaluation mode to evaluate loss on the validation set\n",
    "  model.eval()\n",
    "\n",
    "  # Tracking variables \n",
    "  eval_loss, eval_accuracy = 0, 0\n",
    "  nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "  # Evaluate data for one epoch\n",
    "  for batch in validation_dataloader:\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "    with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "    \n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "    \n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "    nb_eval_steps += 1\n",
    "\n",
    "  print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"saved_model\")\n",
    "\n",
    "import pickle\n",
    "with open(\"saved.pickle\", \"wb\") as f:\n",
    "    pickle.dump({'input_ids': input_ids,\n",
    "                'attention_mask': attention_masks}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4FI9tYIuWTgi"
   },
   "outputs": [],
   "source": [
    "STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Thfec3bVy8d"
   },
   "source": [
    "## Training evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y1Bw98-mU2GB"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "plt.title(\"Training loss\")\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(train_loss_set)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CspMgBR4U2eM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SU9BtumTU2wO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DuXJoDJx8cN-"
   },
   "source": [
    "## STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "69fJZL26Q3FL",
    "outputId": "f827de47-e61b-4230-8946-4d62360e1f72"
   },
   "outputs": [],
   "source": [
    "bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8' \n",
    "\n",
    "map_name_to_handle = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/google/electra_small/2',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/google/electra_base/2',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
    "}\n",
    "\n",
    "map_model_to_preprocess = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_preprocess/2',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "}\n",
    "\n",
    "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
    "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
    "\n",
    "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
    "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 572
    },
    "id": "lFF9pFwIOiyQ",
    "outputId": "bf1b1cec-e79e-45c6-8dd9-0afdbf527c41"
   },
   "outputs": [],
   "source": [
    "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uEtSnYYCQYIz"
   },
   "outputs": [],
   "source": [
    "text_test = ['this is such an amazing movie!']\n",
    "text_preprocessed = bert_preprocess_model(text_test)\n",
    "\n",
    "print(f'Keys       : {list(text_preprocessed.keys())}')\n",
    "print(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')\n",
    "print(f'Word Ids   : {text_preprocessed[\"input_word_ids\"][0, :12]}')\n",
    "print(f'Input Mask : {text_preprocessed[\"input_mask\"][0, :12]}')\n",
    "print(f'Type Ids   : {text_preprocessed[\"input_type_ids\"][0, :12]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j4mqsrHxTFcm"
   },
   "outputs": [],
   "source": [
    "model_checkpoint = \"bert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, truncation=True, padding='max_length', return_special_tokens_mask=True)\n",
    "\n",
    "text_df = pd.DataFrame({'Text':text})\n",
    "\n",
    "#initiating model\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)\n",
    "model.to(device)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer)\n",
    "args = TrainingArguments(\n",
    "    save_path),\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-3,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    "    per_device_train_batch_size = 8,#256,\n",
    "    per_device_eval_batch_size = 8,#256,\n",
    "    logging_steps=50,\n",
    "    eval_steps = 50,\n",
    "    save_total_limit = 3, #saves only last 3 checkpoints\n",
    "    gradient_accumulation_steps=32,#64,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    optim=\"adafactor\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IvhE5Lh7TnwO",
    "outputId": "941f51ba-9578-486f-e011-11f7a5581d8d"
   },
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YuV78h3qTGJ_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
